---
title: "Accelerating Reinforcement Learning for Reaching Using Continuous Curriculum Learning"
collection: publications
permalink: /publication/IJCNN2020
excerpt: "Reinforcement learning has shown great promise in the training of robot behavior due to the sequential decision
making characteristics. However, the required enormous amount of interactive and informative training data provides the major
stumbling block for progress. In this study, we focus on accelerating reinforcement learning (RL) training and improving the
performance of multi-goal reaching tasks. Specifically, we propose a precision-based continuous curriculum learning (PCCL)
method in which the requirements are gradually adjusted during the training process, instead of fixing the parameter in a static
schedule. To this end, we explore various continuous curriculum strategies for controlling a training process. This approach is
tested using a Universal Robot 5e in both simulation and realworld multi-goal reach experiments. Experimental results support the hypothesis that a static training schedule is suboptimal, and using an appropriate decay function for curriculum learning provides superior results in a faster way." 
date: 2020-10-01
venue: 'IEEE International Joint Conference on Neural Networks (IJCNN)'
paperurl: 'https://ieeexplore.ieee.org/document/9207427'
---

Reinforcement learning has shown great promise in the training of robot behavior due to the sequential decision
making characteristics. However, the required enormous amount of interactive and informative training data provides the major
stumbling block for progress. In this study, we focus on accelerating reinforcement learning (RL) training and improving the
performance of multi-goal reaching tasks. Specifically, we propose a precision-based continuous curriculum learning (PCCL)
method in which the requirements are gradually adjusted during the training process, instead of fixing the parameter in a static
schedule. To this end, we explore various continuous curriculum strategies for controlling a training process. This approach is
tested using a Universal Robot 5e in both simulation and realworld multi-goal reach experiments. Experimental results support the hypothesis that a static training schedule is suboptimal, and using an appropriate decay function for curriculum learning provides superior results in a faster way.

[Download paper here](https://arxiv.org/pdf/2002.02697.pdf)
 
 <iframe
    width="426"
    height="320"
    src="https://www.youtube.com/embed/WY-1EbYBSGo"
    frameborder="0"
    allow="autoplay; encrypted-media"
    allowfullscreen
>
</iframe>
